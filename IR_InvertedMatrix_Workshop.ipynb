{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890288b1",
   "metadata": {},
   "source": [
    "# ðŸ› ï¸ Active Learning Workshop: Implementing an Inverted Matrix (Jupyter + GitHub Edition)\n",
    "## ðŸ” Workshop Theme\n",
    "*Readable, correct, and collaboratively reviewed codeâ€”just like in the real world.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e78d18",
   "metadata": {},
   "source": [
    "\n",
    "Welcome to the 90-minute workshop! In this hands-on session, your team will build an **Inverted Index** pipeline, the foundation of many intelligent systems that need fast and relevant access to text data â€” such as AI agents.\n",
    "\n",
    "### ðŸ‘¥ Team Guidelines\n",
    "- Work in teams of 3.\n",
    "- Submit one completed Jupyter Notebook per team.\n",
    "- The final notebook must contain **Markdown explanations** and **Python code**.\n",
    "- Push your notebook to GitHub and share the `.git` link before class ends.\n",
    "\n",
    "---\n",
    "## ðŸ”§ Workshop Tasks Overview\n",
    "\n",
    "1. **Document Collection**\n",
    "2. **Tokenizer Implementation**\n",
    "3. **Normalization Pipeline (Stemming, Stop Words, etc.)**\n",
    "4. **Build and Query the Inverted Index**\n",
    "\n",
    "> Each step includes a sample **talking point**. Your team must add your own custom **Markdown + code cells** with a **second talking point**, and test your Inverted Index with **2 phrase queries**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a922333",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ðŸ§  Learning Objectives\n",
    "- Implement an **Inverted Matrix** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## ðŸ§© Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(15 min)* â€“ Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(45 min)* â€“ Manual IR and Inverted Matrix coding + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(15 min)* â€“ Teams commit and push initial notebooks. **Make sure to include your names so it is easy to identify the team that developed the Min-Max code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(15 min)* â€“ Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - Inverted Matrix  Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## ðŸ’» Submission Checklist\n",
    "- âœ… `IR_InvertedMatrix_Workshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, and Inverted Index.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** and 2 phrase query tests\n",
    "- âœ… `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- âœ… GitHub Repo:\n",
    "  - Public repo named `IR-invertedmatrix-workshop`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e160c9d",
   "metadata": {},
   "source": [
    "## ðŸ“„ Step 1: Document Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc964464",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> We begin by gathering a text corpus. To build a robust index, your vocabulary should include **over 2000 unique words**. You can use scraped articles, academic papers, or open datasets.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Collect at least 20+ text documents.\n",
    "- Ensure the vocabulary exceeds 2000 unique words.\n",
    "- Load the documents into a list for processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d287c34",
   "metadata": {},
   "source": [
    "## Downloading the documents from an online source and saving them locally (sample_docs folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23ee0c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded book ID 1342\n",
      "Downloaded book ID 84\n",
      "Downloaded book ID 11\n",
      "Downloaded book ID 2701\n",
      "Downloaded book ID 1661\n",
      "Downloaded book ID 76\n",
      "Downloaded book ID 98\n",
      "Downloaded book ID 1232\n",
      "Downloaded book ID 2542\n",
      "Downloaded book ID 174\n",
      "Downloaded book ID 5200\n",
      "Downloaded book ID 158\n",
      "Downloaded book ID 1260\n",
      "Downloaded book ID 43\n",
      "Downloaded book ID 1080\n",
      "Downloaded book ID 120\n",
      "Downloaded book ID 16328\n",
      "Downloaded book ID 1400\n",
      "Downloaded book ID 996\n",
      "Downloaded book ID 1952\n",
      "Downloaded book ID 345\n",
      "Downloaded book ID 2852\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Create folder\n",
    "folder_name = \"sample_docs\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# List of Project Gutenberg book IDs (you can add more)\n",
    "book_ids = [\n",
    "    1342, 84, 11, 2701, 1661, 76, 98, 1232, 2542, 174, \n",
    "    5200, 158, 1260, 43, 1080, 120, 16328, 1400, 996, \n",
    "    1952, 345, 2852\n",
    "]\n",
    "\n",
    "# Download each book\n",
    "for book_id in book_ids:\n",
    "    url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(f\"{folder_name}/book_{book_id}.txt\", \"w\", encoding='utf-8') as f:\n",
    "                f.write(response.text)\n",
    "            print(f\"Downloaded book ID {book_id}\")\n",
    "        else:\n",
    "            print(f\"Failed to download book ID {book_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with book ID {book_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b909e2",
   "metadata": {},
   "source": [
    "To collect enough text data, we downloaded 22 public domain books from Project Gutenberg (https://www.gutenberg.org) using their book IDs. The script saves each book as a .txt file in the sample_docs folder.\n",
    "\n",
    "From the output we can se that 22 documents were downloaded successfuly and only one of them failed to download. They are all books with IDs generated by an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf21591d",
   "metadata": {},
   "source": [
    "## Loading the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "264d7a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 22 documents.\n"
     ]
    }
   ],
   "source": [
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n",
    "\n",
    "folder = 'sample_docs' \n",
    "documents = load_documents(folder)\n",
    "print(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce1a61d",
   "metadata": {},
   "source": [
    "After we downloaded al the documents and stored them locally in a folder called \"sample_docs\" , now we have to load these documents by suing the load_documents() method. We give the folder as a parameter for the method. We can clearly see from the output that all the documents are loaded successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342945a",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Step 2: Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803fb52",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> The tokenizer breaks raw text into a stream of words (tokens). This is the foundation for every later step in IR and NLP.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Implement a basic tokenizer that splits text into lowercase words.\n",
    "- Handle punctuation removal and basic non-alphanumeric filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4806fc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens across all documents: 2308765\n",
      "Sample tokens: ['start', 'of', 'the', 'project', 'gutenberg', 'ebook', '1080', 'a', 'modest', 'proposal', 'for', 'preventing', 'the', 'children', 'of', 'poor', 'people', 'in', 'ireland', 'from']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def basic_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  \n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "for doc in documents:\n",
    "    tokens = basic_tokenizer(doc)\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "print(f\"Total tokens across all documents: {len(all_tokens)}\")\n",
    "print(f\"Sample tokens: {all_tokens[:20]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da758b71",
   "metadata": {},
   "source": [
    "We load each book from the sample_docs folder and process the text by converting it to lowercase and removing punctuation. This basic tokenization step breaks the text into clean, individual words (tokens). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed76ec",
   "metadata": {},
   "source": [
    "## ðŸ” Step 3: Normalization Pipeline (Stemming, Stop Word Removal, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f277a0d",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> Now we normalize tokens: convert to lowercase, remove stop words, apply stemming or affix stripping. This reduces redundancy and enhances search accuracy.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Use `nltk` to remove stopwords and apply stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66ae9a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalization: ['start', 'of', 'the', 'project', 'gutenberg', 'ebook', '1080', 'a', 'modest', 'proposal']\n",
      "After normalization: ['start', 'project', 'gutenberg', 'ebook', '1080', 'modest', 'propos', 'prevent', 'children', 'poor']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def normalize_tokens(tokens):\n",
    "    filtered = [w for w in tokens if w not in stop_words]         \n",
    "    stemmed = [stemmer.stem(w) for w in filtered]                 \n",
    "    return stemmed\n",
    "\n",
    "\n",
    "tokens = basic_tokenizer(documents[0])  \n",
    "normalized_tokens = normalize_tokens(tokens)\n",
    "\n",
    "print(f\"Before normalization: {tokens[:10]}\")\n",
    "print(f\"After normalization: {normalized_tokens[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277c38a",
   "metadata": {},
   "source": [
    "We remove common stopwords like \"the\" and \"and,\" then simplify words by stemming. For example we turn the word \"running\" into just \"run\". \n",
    "This transformation makes the text cleaner and easier to analyze.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34cf58",
   "metadata": {},
   "source": [
    "## ðŸ” Step 4: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c39dd",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> We now map each normalized token to the list of document IDs in which it appears. This is the core structure that allows fast Boolean and phrase queries.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Build the inverted index using a dictionary.\n",
    "- Add code to support phrase queries using positional indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca8f106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Inverted index built successfully!\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_inverted_index(docs):\n",
    "    inverted_index = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "\n",
    "    for doc_id, doc in enumerate(docs):\n",
    "        tokens = normalize_tokens(basic_tokenizer(doc))\n",
    "        for pos, token in enumerate(tokens):\n",
    "            inverted_index[token][doc_id].append(pos)\n",
    "    print(\"Inverted index built successfully!\")\n",
    "    return inverted_index\n",
    "\n",
    "inverted_index = build_inverted_index(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f590609",
   "metadata": {},
   "source": [
    "We create an inverted index that maps each word to the documents and positions where it appears. This allows us to quickly find words and phrases later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef4df8",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test: Phrase Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db832216",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> A phrase query requires the exact sequence of terms (e.g., \"machine learning\"). To support this, extend the inverted index to store positions, not just docIDs.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Implement 2 phrase queries.\n",
    "- Demonstrate that they return the correct documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97132fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_query(inverted_index, phrase, documents):\n",
    "    phrase_tokens = normalize_tokens(basic_tokenizer(phrase))\n",
    "    if not phrase_tokens:\n",
    "        return set()\n",
    "\n",
    "  \n",
    "    candidate_docs = set(inverted_index.get(phrase_tokens[0], {}).keys())\n",
    "\n",
    "    for token in phrase_tokens[1:]:\n",
    "        candidate_docs &= set(inverted_index.get(token, {}).keys())\n",
    "\n",
    "    matching_docs = set()\n",
    "\n",
    "    for doc_id in candidate_docs:\n",
    "        positions_lists = [inverted_index[token][doc_id] for token in phrase_tokens]\n",
    "\n",
    "       \n",
    "        for pos in positions_lists[0]:\n",
    "            if all((pos + i) in positions_lists[i] for i in range(1, len(phrase_tokens))):\n",
    "                matching_docs.add(doc_id)\n",
    "                break\n",
    "\n",
    "    return matching_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c4446c",
   "metadata": {},
   "source": [
    "This function finds documents that contain an exact phrase (like the ones that are mentionedd in the code cell below) using an inverted index. It:\n",
    "\n",
    "1. Tokenizes the phrase\n",
    "\n",
    "2. Finds docs with all words\n",
    "\n",
    "3. Checks if the words appear next to each other in the right order\n",
    "\n",
    "4. Returns a set of matching document IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1d576ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase query 'great expectations' found in documents: {6}\n",
      "Phrase query 'little women' found in documents: set()\n",
      "Phrase query 'my dear' found in documents: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}\n",
      "Phrase query 'a few minutes' found in documents: {1, 2, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query1 = \"great expectations\"\n",
    "query2 = \"little women\"\n",
    "query3 = \"my dear\"\n",
    "query4 = \"a few minutes\"\n",
    "\n",
    "results1 = phrase_query(inverted_index, query1, documents)\n",
    "results2 = phrase_query(inverted_index, query2, documents)\n",
    "results3 = phrase_query(inverted_index, query3, documents)\n",
    "results4 = phrase_query(inverted_index, query4, documents)\n",
    "\n",
    "print(f\"Phrase query '{query1}' found in documents: {results1}\")\n",
    "print(f\"Phrase query '{query2}' found in documents: {results2}\")\n",
    "print(f\"Phrase query '{query3}' found in documents: {results3}\")\n",
    "print(f\"Phrase query '{query4}' found in documents: {results4}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f403749",
   "metadata": {},
   "source": [
    "###  Phrase Query Results\n",
    "\n",
    "We tested a few phrase queries using our inverted index with positional information. Here's what we found:\n",
    "\n",
    "- \"great expectations\" was found 1 document â€” likely the book itself.\n",
    "- \"little women was not found â€” the book might not be in our collection.\n",
    "- \"my dear\" appeared in 22 documents â€” very common in classic dialogue.\n",
    "- \"a few minutes\" appeared in 18 documents â€” often used in scene transitions.\n",
    "\n",
    "These results show that the phrase search works and reflects common literary patterns across multiple books."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a16721",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "\n",
    "In this lab, we successfully built a simple search system using core NLP techniques. We:\n",
    "\n",
    "- Collected and cleaned a real-world text dataset\n",
    "- Tokenized and normalized the text\n",
    "- Built an inverted index with positional information\n",
    "- Performed phrase queries to find exact word sequences\n",
    "\n",
    "This exercise helped us understand how search engines work under the hood, especially how text is indexed and queried efficiently. It also showed the power of preprocessing in improving search accuracy.\n",
    "\n",
    "Overall, weâ€™ve taken an important step toward understanding real-world information retrieval.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
